{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df688d86-909f-429c-97a0-5af7d6858672",
   "metadata": {},
   "source": [
    "# Before running this notebook, you need to finish the \"preprocess_waymo\" notebook first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36741b9f-2927-4709-9370-dfb249e5d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import utils.utils as utils\n",
    "import utils.dataloader_utils as dataloader_utils\n",
    "import utils.train_utils as train_utils\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768871e7-16c0-4d02-a470-fc590a8e8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need GPU to run this notebook\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0d26d-5c2f-438f-8b11-c7a8df8c901c",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f63899-b64e-4e33-ab55-e5230eef71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "num_points_in_one_traj = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4895b5-ec71-43df-8c65-547514ac8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/ego/ego_trajs_\" + str(num_points_in_one_traj) + \"_json/ego_trajs_outlier_indicies.json\", \"r\") as read_file:\n",
    "    idx_invalid_idx = set(json.load(read_file))\n",
    "\n",
    "list_dataset = dataloader_utils.generate_file_list_dataset('data/ego/ego_trajs_json/', idx_invalid_idx)\n",
    "start_idx_dataset = dataloader_utils.generate_start_indicies_dataset(\"data/ego/ego_trajs_\" + str(num_points_in_one_traj) + \"_json/ego_trajs_start_point_indicies.json\", idx_invalid_idx)\n",
    "combined_dataset = tf.data.Dataset.zip((list_dataset, start_idx_dataset))\n",
    "\n",
    "dataProcessor = dataloader_utils.DataProcessor(BATCH_SIZE, combined_dataset, num_points_in_one_traj, traj_type = 'ego_traj')\n",
    "dataProcessor.load_process(shuffle = True)\n",
    "\n",
    "print(dataProcessor.loaded_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155b5ec-ebc4-4ca4-a8fb-5f91c1b7650c",
   "metadata": {},
   "source": [
    "## Step 2: Empirical Bayes Analysis for Ego Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b7afe70-4857-485f-8605-366c862d0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# longer trajectories need more epochs\n",
    "EPOCHS = 50 \n",
    "\n",
    "# longer trajectories need lower lr, consider using https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PiecewiseConstantDecay\n",
    "lr = 5e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66ef63-d97e-406f-bcb7-75266f80fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "best_epoch_losses = []\n",
    "best_epochs = []\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "A_list, B_list = [], []\n",
    "optimizers_ser = []\n",
    "log_root_dir = 'logs/gradient_tape/ego/ego_xy' + str(num_points_in_one_traj)\n",
    "t_scale_factor = (num_points_in_one_traj-1) / 10 # The time duration of one trajectory, for scaling time to interval (0,1)\n",
    "degrees = np.linspace(1, 8, 8, dtype=np.int16) # analyse polynomials from degree 1 to 8\n",
    "\n",
    "for i_d, deg in enumerate(degrees):\n",
    "    print('Analysing Deg ',deg)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)   \n",
    "    optimizers_ser.append(tf.keras.optimizers.serialize(optimizer))\n",
    "    \n",
    "    # Initialize all trainable variables\n",
    "    A = tf.Variable(np.random.randn(2*(deg), 2*(deg)) * 1e-1 , dtype=tf.float64, name='alpha') # Model uncertainty. Note: 0-th parameter (start position) is not interested\n",
    "    B_diag = tf.Variable(np.random.randn(1) * 1e-1, dtype=tf.float64, name='beta_diag') # Log of Observation uncertainty\n",
    "    B_by_diag =  tf.Variable(np.random.randn(1) * 1e-1, dtype=tf.float64, name='beta_by_diag')\n",
    "\n",
    "    \n",
    "    train_log_dir = log_root_dir + '/deg_' + str(deg)\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)  \n",
    "   \n",
    "    model_losses, best_epoch_loss, best_epoch, best_alpha, best_beta_diag, best_beta_by_diag = train_utils.train_ego(alpha=A, beta_diag=B_diag, beta_by_diag=B_by_diag, t_scale_factor = t_scale_factor, degree = deg,\n",
    "                                                                                                                     opti=optimizer, epochs = EPOCHS, data_loader=dataProcessor.loaded_dataset, tf_summary_writer = train_summary_writer, \n",
    "                                                                                                                     verbose = True, early_stop=False)\n",
    "            \n",
    "    # Add model loss\n",
    "    losses.append(model_losses)\n",
    "    best_epoch_losses.append([best_epoch_loss])\n",
    "    \n",
    "    # store the best epoch\n",
    "    best_epochs.append(best_epoch)\n",
    "    \n",
    "    # Compute the AIC and BIC score\n",
    "    aic_score, bic_score = utils.compute_AIC_BIC(nll = best_epoch_loss, deg = deg, num_points = num_points_in_one_traj, dof_in_ob = 2)\n",
    "\n",
    "    bic_scores.append(bic_score)\n",
    "    aic_scores.append(aic_score)\n",
    "    \n",
    "    # Compute the model uncertainty, A_unscaled = np.linalg.inv(scale_mat) @ A_scaled\n",
    "    A_scale_mat = utils.polyBasisScale(t_scale_factor, deg)\n",
    "    A_scale_mat = A_scale_mat[1:, 1:]\n",
    "    A_est = np.linalg.inv(np.kron(np.eye(2), A_scale_mat)) @ best_alpha.numpy()\n",
    "    A_est = A_est @ A_est.T\n",
    "    A_list.append(A_est)\n",
    "    \n",
    "    # Compute the observation uncertainty, B_cov = tf.eye(num_points_in_one_traj) * tf.math.softplus(B)\n",
    "    B_est = {'B_diag': (tf.math.softplus(best_beta_diag)).numpy(), \n",
    "             'B_by_diag': (tf.math.softplus(best_beta_diag) * tf.math.tanh(best_beta_by_diag)).numpy()}\n",
    "    B_list.append(B_est)\n",
    "    print(\"Degree: {}, NLL: {}, BIC: {}, AIC: {}\".format(deg, model_losses[-1], bic_score, aic_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a60c6-bbe8-4b9d-bdc3-9c9cad925f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = utils.calculate_result(degrees, bic_scores, aic_scores, A_list, B_list, best_epoch_losses, best_epochs, lr, optimizers_ser, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68f64f-e682-4c66-aa5b-dfdbc0336d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_result(folder_dir =log_root_dir, file_name='result_summary', result=result)\n",
    "with open(log_root_dir + '/' + 'optimizers' + '.json', \"w\") as write_file:\n",
    "    json.dump(optimizers_ser, write_file, cls=NumpyEncoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p38)",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
